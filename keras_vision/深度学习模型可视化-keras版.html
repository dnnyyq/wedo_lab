<!DOCTYPE html>
<html>
<head>
<title>深度学习模型可视化-keras版.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96-keras%E7%89%88">深度学习模型可视化-keras版</h1>
<h2 id="1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%AF%E8%A7%86%E5%8C%96">1. 深度学习可视化</h2>
<p>深度学习的过程是一个黑盒子，模型通过大量的权重去学习拟合输入的数据和学习目标，模型的性能很大程度上取决于模型的输入的数据； 深度学习的拟合效果往往出乎我们的的想象，但是模型如何拟合数据和学习目标之间的关系，我们知之甚少。</p>
<p>有时候训练数据和验证集的选取，模型真正学习到的东西和我们人类认知背道而驰。网上看到的一则案例：有人采集了100张隐藏在树丛中的坦克照片，以及另100张仅有树丛的照片， 用神经网络训练一个识别坦克的分类器，在训练和验证上模型都达到了100%的精确度，（100%基本上是数据泄露了）; 原因出在100张坦克是在晴天拍摄，另外100张是阴天拍摄，模型似乎只关注到了天空的颜色。</p>
<p>本文针对keras对常用的机器视觉的可视化做了总结，</p>
<ul>
<li>特征可视化</li>
<li>Grad-CAM类激活热力图</li>
</ul>
<h2 id="2-%E5%8F%AF%E8%A7%86%E5%8C%96%E6%A8%A1%E5%9E%8B%E8%AF%B4%E6%98%8E">2. 可视化模型说明</h2>
<p>本文要可视化模型为densnet121，其他模型类似，只是卷积层不一样，目标是对于火灾的识别</p>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> keras.applications <span class="hljs-keyword">import</span> DenseNet169
<span class="hljs-keyword">from</span> keras.preprocessing <span class="hljs-keyword">import</span> image
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense, GlobalAveragePooling2D
<span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K

<span class="hljs-comment"># 构建不带分类器的预训练模型</span>
base_model = DenseNet169(weights=<span class="hljs-string">'imagenet'</span>, include_top=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># 添加全局平均池化层</span>
x = base_model.output
x = GlobalAveragePooling2D()(x)

<span class="hljs-comment"># 添加一个全连接层</span>
x = Dense(<span class="hljs-number">1024</span>, activation=<span class="hljs-string">'relu'</span>)(x)

<span class="hljs-comment"># 添加一个分类器，假设我们有200个类</span>
predictions = Dense(<span class="hljs-number">200</span>, activation=<span class="hljs-string">'softmax'</span>)(x)

<span class="hljs-comment"># 构建我们需要训练的完整模型</span>
model = Model(inputs=base_model.input, outputs=predictions)
</div></code></pre>
<p>查看模型结构</p>
<pre class="hljs"><code><div>model.summary()
</div></code></pre>
<p><img src="img/md-2020-07-03-23-39-19.png" alt=""></p>
<h2 id="3-%E7%89%B9%E5%BE%81%E5%8F%AF%E8%A7%86%E5%8C%96">3. 特征可视化</h2>
<p>对每一层卷积核的可视化帮助我们了解算法抽取的特征情况
可以通过keras中的<code>K.function</code>封装输入到输出的函数，获取指定层的输出</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 训练好的模型为model.h5</span>
<span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># 加载模型</span>
model_ = load_model(<span class="hljs-string">'./model.h5'</span>)
<span class="hljs-comment"># 设置为测试阶段</span>
K.set_learning_phase(<span class="hljs-number">0</span>)
graph = tf.get_default_graph()

img_file = <span class="hljs-string">'./test.jpg'</span>
img = cv2.imread(img_file)
img = cv2.resize(img, (<span class="hljs-number">224</span>, <span class="hljs-number">224</span>))
img = img.astype(<span class="hljs-string">'float32'</span>)
img = img / <span class="hljs-number">255.0</span> * <span class="hljs-number">2</span> - <span class="hljs-number">1</span>
img = np.expand_dims(img, <span class="hljs-number">0</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_layer_feat_byname</span><span class="hljs-params">(graph, img, model_, layer_name=<span class="hljs-string">'conv1/relu'</span>)</span>:</span>
    <span class="hljs-keyword">with</span> graph.as_default():
        layer_fn = K.function([model_.layers[<span class="hljs-number">0</span>].input, K.learning_phase()], [model_.get_layer(layer_name).output])
        layer_output = layer_fn([img, <span class="hljs-number">0</span>])[<span class="hljs-number">0</span>]
        <span class="hljs-keyword">return</span> layer_output

layer_output1 = get_layer_feat_byname(graph, img, model_, <span class="hljs-string">'conv1/relu'</span>)
layer_output2 = get_layer_feat_byname(graph, img, model_, <span class="hljs-string">'pool2_conv'</span>)
layer_output3 = get_layer_feat_byname(graph, img, model_, <span class="hljs-string">'pool3_conv'</span>)
layer_output4 = get_layer_feat_byname(graph, img, model_, <span class="hljs-string">'pool4_conv'</span>)
layer_output5 = get_layer_feat_byname(graph, img, model_, <span class="hljs-string">'conv5_block32_concat'</span>)

</div></code></pre>
<p>对于densenet169， 我们可以选择每个dense_block层的最后一个concat，也可以选择transition_block pooling前面的卷积层做展示，当然每一个卷积层都是可以做展示的，卷积层名称可以在<code>summary()</code>可以查到。本文 <code>conv1/relu</code> ,<code>pool2_conv</code>,<code>pool3_conv</code>,<code>pool4_conv</code>和最后的<code>conv5_block32_concat</code></p>
<p>我们看下特征的可视化例子</p>
<ul>
<li>原图：来自网络</li>
</ul>
<p><img src="img/md-2020-07-04-22-15-35.png" alt=""></p>
<ul>
<li>特征可视化:依次是<code>conv1/relu</code> ,<code>pool2_conv</code>,<code>pool3_conv</code>,<code>pool4_conv</code>和最后的<code>conv5_block32_concat</code></li>
</ul>
<p><img src="img/md-2020-07-04-22-16-54.png" alt="">
<img src="img/md-2020-07-04-22-17-40.png" alt="">
<img src="img/md-2020-07-04-22-17-55.png" alt="">
<img src="img/md-2020-07-04-22-18-26.png" alt="">
<img src="img/md-2020-07-04-22-18-45.png" alt=""></p>
<p>可以看出</p>
<ul>
<li>浅层的卷积特征主要形状和纹理</li>
<li>层数越深，特征越少，也也抽象</li>
<li>到最后一层卷积，可以看出模型主要的关注响应点，可以和人为主观上做一个对照，看一下模型识别到的是否目标真正的意图。</li>
<li>可以对输入图像做一定处理，比如遮罩掉一部分，看看特征的响应</li>
<li>如果看到过多的无用特征，只有少部分特征，可以考虑加下dropout，看是否能提升性能</li>
</ul>
<h2 id="4-grad-cam%E7%B1%BB%E6%BF%80%E6%B4%BB%E7%83%AD%E5%8A%9B%E5%9B%BE">4. Grad-CAM类激活热力图</h2>
<p>由于每一层的特征数较多，只能初略观察下对目标的响应情况。而具体某个类别对应到图片的那个区域响应最大，也就是对该类别的识别贡献最大，没有一个直观的可视化。2016年这篇文章给出了很好的解决方案，而且实现比较简单，Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization。
<img src="img/md-2020-07-04-21-44-32.png" alt=""></p>
<p>Grad-CAM思想来源CAM（Class Activation Mapping），区别在于计算特征的响应权重不同，CAM采用GAP层后的所有权重，因而CAM必须要有GAP层。而Grad-CAM采用目标类别对特征的梯度来作为响应权重， 对所有网络结构都适用。</p>
<p><img src="img/md-2020-07-04-21-52-34.png" alt=""></p>
<p>过程描述</p>
<ul>
<li>获取最后一个卷积层</li>
<li>获取目标类别输出</li>
<li>计算目标类别对卷积特征的梯度（可以考虑下梯度的含义, 下降最快，响应最大的）</li>
<li>同样是用K.function建立输入和输出的函数</li>
<li>计算特征和权重的相乘，并求全局平均</li>
<li>计算一个relu， 映射到原图大小</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">output_heatmap</span><span class="hljs-params">(model, last_conv_layer, img)</span>:</span>
    <span class="hljs-string">"""Get the heatmap for image.

    Args:
           model: keras model.
           last_conv_layer: name of last conv layer in the model.
           img: processed input image.

    Returns:
           heatmap: heatmap.
    """</span>
    <span class="hljs-comment"># predict the image class</span>

    preds = model.predict(img)
    <span class="hljs-comment"># find the class index</span>
    index = np.argmax(preds[<span class="hljs-number">0</span>])
    print(<span class="hljs-string">'index: %s'</span> % index)
    <span class="hljs-comment"># This is the entry in the prediction vector</span>
    target_output = model.output[:, index]

        
        
    <span class="hljs-comment"># get the last conv layer</span>
    last_conv_layer = model.get_layer(last_conv_layer)

    <span class="hljs-comment"># compute the gradient of the output feature map with this target class</span>
    grads = K.gradients(target_output, last_conv_layer.output)[<span class="hljs-number">0</span>]

    <span class="hljs-comment"># mean the gradient over a specific feature map channel</span>
    pooled_grads = K.mean(grads, axis=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>))

    <span class="hljs-comment"># this function returns the output of last_conv_layer and grads </span>
    <span class="hljs-comment"># given the input picture</span>
    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[<span class="hljs-number">0</span>]])
    pooled_grads_value, conv_layer_output_value = iterate([img])

    <span class="hljs-comment"># We multiply each channel in the feature map array</span>
    <span class="hljs-comment"># by "how important this channel is" with regard to the target class</span>

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(conv_layer_output_value.shape[<span class="hljs-number">-1</span>]):
        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]

    <span class="hljs-comment"># The channel-wise mean of the resulting feature map</span>
    <span class="hljs-comment"># is our heatmap of class activation</span>
    heatmap = np.mean(conv_layer_output_value, axis=<span class="hljs-number">-1</span>)
    
    heatmap = cv2.resize(heatmap, (img.shape[<span class="hljs-number">1</span>], img.shape[<span class="hljs-number">2</span>]), cv2.INTER_LINEAR)

    heatmap = np.maximum(heatmap, <span class="hljs-number">0</span>)
    heatmap /= np.max(heatmap)
    print(heatmap.shape)
    <span class="hljs-keyword">return</span> heatmap, index
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> cv2

<span class="hljs-comment"># 加载模型</span>
model_ = load_model(<span class="hljs-string">'./model.h5'</span>)
<span class="hljs-comment"># 设置为测试阶段</span>
K.set_learning_phase(<span class="hljs-number">0</span>)
graph = tf.get_default_graph()

img_file = <span class="hljs-string">'./test.jpg'</span>
img = cv2.imread(img_file)
img = cv2.resize(img, (<span class="hljs-number">224</span>, <span class="hljs-number">224</span>))
img = img.astype(<span class="hljs-string">'float32'</span>)
img = img / <span class="hljs-number">255.0</span> * <span class="hljs-number">2</span> - <span class="hljs-number">1</span>
img = np.expand_dims(img, <span class="hljs-number">0</span>)

heatmap, index = output_heatmap(model_, <span class="hljs-string">'conv5_block32_concat'</span>, img)

</div></code></pre>
<p>我们来看一下效果</p>
<p><img src="img/md-2020-07-04-22-20-57.png" alt=""></p>
<h2 id="5-%E6%80%BB%E7%BB%93">5. 总结</h2>
<p>本文演示了keras在深度学习可视化的两种方式，希望对你有帮助，欢迎交流@mintel。</p>
<p>总结如下</p>
<ul>
<li>使用summary查看layer名称</li>
<li>使用K.function和model.get_layer 建立模型输入和输出, 进行特征可视化</li>
<li>Grad-CAM简单高效的类别响应可视化，图像的哪些像素决定了类型输出。关键在于类别输出对特征的梯度</li>
</ul>

</body>
</html>
