<!DOCTYPE html>
<html>
<head>
<title>horovod.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="%E6%8F%90%E5%8D%87%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7gpu%E5%88%A9%E7%94%A8%E7%8E%87%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83horovod">提升深度学习训练技巧：GPU利用率和分布式训练Horovod</h1>
<p>个人简介：
wedo实验君, 数据分析师；热爱生活，热爱写作</p>
<h2 id="1-%E8%AE%AD%E7%BB%83%E7%9A%84%E7%93%B6%E9%A2%88%E5%9C%A8%E5%93%AA%E9%87%8C">1. 训练的瓶颈在哪里</h2>
<ul>
<li>
<p>GPU利用率低：模型训练时GPU显存沾满了，但是GPU的利用率比较不稳定，有时候0%，有时候90%，忽高忽低。</p>
<p><img src="img/md-2020-09-19-00-46-07.png" alt=""></p>
</li>
<li>
<p>训练的数据量大：训练数据大，在百万/千万的量级，训练一个Epoch需要很长时间，模型迭代周期过长。</p>
</li>
</ul>
<h2 id="2-%E6%8F%90%E9%AB%98gpu%E5%88%A9%E7%94%A8%E7%8E%87cpu-vs-gpu">2. 提高GPU利用率：CPU vs GPU</h2>
<p>GPU利用率低, 主要原因是CPU处理的效率跟不上GPU</p>
<h3 id="21-cpu-vs-gpu%E7%9A%84%E9%80%9A%E4%BF%A1">2.1 CPU vs GPU的通信</h3>
<ul>
<li>CPU负责加载数据+数据预处理，并不断的在内存和显存之间交互数据</li>
<li>GPU负责模型训练（图片来自网络）</li>
</ul>
<p><img src="img/md-2020-09-18-15-21-56.png" alt=""></p>
<p><img src="img/md-2020-09-18-15-22-27.png" alt=""></p>
<h3 id="22-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">2.2 解决方案</h3>
<p>采用多进程并行处理，加快CPU加载数据的性能</p>
<ul>
<li>keras
keras 中提供了<code>workers</code> <code>use_multiprocessing</code>来采用多进程方式，并行处理数据，并push到队列中，共GPU模型训练。因为进程之间可能相互影响资源，并不是越大越好，workers可以设置2，4，8。<pre class="hljs"><code><div>run_model.fit_generator(
            generator=training_generator,
            class_weight={<span class="hljs-number">0</span>: config.weights, <span class="hljs-number">1</span>: <span class="hljs-number">1</span>},
            epochs=epochs,
            verbose=<span class="hljs-number">1</span>,
            steps_per_epoch=steps_per_epoch,
            callbacks=callbacks_list,
            validation_data=valid_generator,
            validation_steps=validation_steps,
            shuffle=<span class="hljs-literal">True</span>,
            workers=<span class="hljs-number">8</span>,
            use_multiprocessing=<span class="hljs-literal">True</span>,
            max_queue_size=<span class="hljs-number">20</span>
</div></code></pre>
</li>
<li>pytorch
torch在加载数据中提供类似参数<code>num_workers</code>。<code>pin_memory=True</code>可以直接加载到显存中，而不需要内存<pre class="hljs"><code><div>torch.utils.data.DataLoader(image_datasets[x],
                            batch_size=batch_size, 
                            shuffle=<span class="hljs-literal">True</span>,
                            num_workers=<span class="hljs-number">8</span>,
                            pin_memory=<span class="hljs-literal">True</span>)
</div></code></pre>
</li>
</ul>
<h2 id="3-%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83">3. 分布式并行训练</h2>
<h3 id="31-%E5%B9%B6%E8%A1%8C%E6%A8%A1%E5%BC%8F">3.1 并行模式</h3>
<p>当训练的数据量很大时，可以通过多个机器多个GPU来提高训练的效率。不同于hadoop和spark等分布式数据处理框架，深度学习训练因为要涉及参数的前项传播和反向传播，有两种并行方式：</p>
<ul>
<li>模型并行（ model parallelism ）:分布式系统中的不同机器（GPU/CPU等）负责网络模型的不同部分，通常是神经网络模型的不同网络层被分配到不同的机器，或者同一层内部的不同参数被分配到不同机器。一般是超大的模型，一张显卡放不下的情况，如NLP的模型。
模型并行的缺点是层和层之间可能存在依赖关系，不能完全的并行。（图片来自网络）</li>
</ul>
<p><img src="img/md-2020-09-18-15-28-56.png" alt=""></p>
<ul>
<li>数据并行（ data parallelism ）：不同的机器有同一个模型的多个副本，每个机器分配到不同的数据，然后将所有机器的计算结果按照某种方式合并。这种就比较适合大数据的情况。数据并行要解决的问题是数据的分割和传输，以及参数的更新。</li>
</ul>
<p><img src="img/md-2020-09-18-15-29-41.png" alt="">（图片来自网络）</p>
<h3 id="32-%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C">3.2 数据并行</h3>
<blockquote>
<p>Facebook在《Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour》介绍了使用 256 块 GPU 进行 ResNet-50 网络「数据并行」训练的方法</p>
</blockquote>
<ul>
<li>数据分割: 选用大的batch-size, 按照worker数量进行分割， 分发到不同worker执行</li>
<li>参数更新： 参数的更新有两种模式（1）参数服务器 （2） ring环状更新（无服务器模式）</li>
</ul>
<h4 id="321-%E5%8F%82%E6%95%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%A8%A1%E5%BC%8F">3.2.1 参数服务器模式</h4>
<p>参数服务器模式，见下图。在每个worker执行完一个batch的训练后，反向传播参数的时候，所有的worker都会把参数传给参数服务器，进行汇总求均值，之后再传给每个worker，进入第二个batch的训练。（图片来自网络）
<img src="img/md-2020-09-18-15-34-50.png" alt=""></p>
<p>参数服务器有一个或者多个的结构模式，可以看出这种数据并行的模式效率是否提升取决于参数服务器与worker之间的通信效率，也就是最慢的worker的训练时间和参数服务器的接收和更新参数后再回传的时间。worker数量多的话，参数服务器可能存在瓶颈。（图片来自网络）</p>
<p><img src="img/md-2020-09-18-15-36-51.png" alt=""></p>
<h4 id="322-ring-reduce">3.2.2 ring-reduce</h4>
<p>百度提出的<code>ring-reduce</code>摒弃了参数服务器，采用环状结构来更新参数。ring-reduce把所有的worker组成一个两两相邻的环形结构。每个worker只与相邻的worker交换参数。经过几次交换之后，所有的worker都包含其他worker的参数信息，达到更新的目的。（图片来自网络）</p>
<p><img src="img/md-2020-09-18-15-35-49.png" alt=""></p>
<p>下面几张图，可以看到其中的几个步骤； <code>ring-reduce</code>为了加快速度，并不是一次性交换所有的参数；而是先把参数进行分割，不断交换分割后参数。
<img src="img/md-2020-09-18-15-41-32.png" alt=""></p>
<p><img src="img/md-2020-09-18-15-41-49.png" alt=""></p>
<p><img src="img/md-2020-09-18-15-42-17.png" alt=""></p>
<h2 id="4-%E5%AE%9E%E7%8E%B0%E6%A1%86%E6%9E%B6horovod">4. 实现框架：Horovod</h2>
<p>Horovod 是 Uber 开源的又一个深度学习工具，它的发展吸取了 Facebook「一小时训练 ImageNet 论文」与百度 Ring Allreduce 的优点，可为用户实现分布式训练提供帮助。<code>https://github.com/horovod/horovod</code></p>
<p>采用NCCL 替换百度的 ring-allreduce 实现。NCCL 是英伟达的集合通信库，提供高度优化的 ring-allreduce 版本。NCCL 2 允许在多个机器之间运行 ring-allreduc。</p>
<p>如果要把单机的训练代码修改成分布式的代码，只要几个步骤就可以了
改造分布式训练：</p>
<ul>
<li>
<p>horovod安装
建议安装docker的horovod，省去安装环境的麻烦。horovod依赖<code>NCCL 2</code> <code>open MPI</code></p>
<pre class="hljs"><code><div>$ mkdir horovod-docker-gpu
$ wget -O horovod-docker-gpu/Dockerfile https://raw.githubusercontent.com/horovod/horovod/master/Dockerfile.gpu
$ docker build -t horovod:latest horovod-docker-gpu
</div></code></pre>
</li>
<li>
<p>机器worker机器之间ssh打通</p>
</li>
<li>
<p>修改训练代码
horovod支持tf,keras,pytorch和mxnet等不同的深度学习框架。以keras为例，修改主要6个步骤
（1） 初始化：hvd.init()
（2）分配GPU计算资源：<code>config.gpu_options.visible_device_list = str(hvd.local_rank())</code>
（3）分布式的优化器来实现参数的分布式更新：<code>opt = hvd.DistributedOptimizer(opt)</code>
（4）定义所有worker模型初始化一致性 <code> hvd.callbacks.BroadcastGlobalVariablesCallback(0)</code>
（5）模型保存在某一个worker</p>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function
<span class="hljs-keyword">import</span> keras
<span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> mnist
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense, Dropout, Flatten
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Conv2D, MaxPooling2D
<span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> horovod.keras <span class="hljs-keyword">as</span> hvd

<span class="hljs-comment"># Horovod: initialize Horovod.</span>
hvd.init()

<span class="hljs-comment"># Horovod: pin GPU to be used to process local rank (one GPU per process)</span>
config = tf.ConfigProto()
config.gpu_options.allow_growth = <span class="hljs-literal">True</span>
config.gpu_options.visible_device_list = str(hvd.local_rank())
K.set_session(tf.Session(config=config))

batch_size = <span class="hljs-number">128</span>
num_classes = <span class="hljs-number">10</span>

<span class="hljs-comment"># Horovod: adjust number of epochs based on number of GPUs.</span>
epochs = int(math.ceil(<span class="hljs-number">12.0</span> / hvd.size()))

<span class="hljs-comment"># Input image dimensions</span>
img_rows, img_cols = <span class="hljs-number">28</span>, <span class="hljs-number">28</span>

<span class="hljs-comment"># The data, shuffled and split between train and test sets</span>
(x_train, y_train), (x_test, y_test) = mnist.load_data()

<span class="hljs-keyword">if</span> K.image_data_format() == <span class="hljs-string">'channels_first'</span>:
    x_train = x_train.reshape(x_train.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>, img_rows, img_cols)
    input_shape = (<span class="hljs-number">1</span>, img_rows, img_cols)
<span class="hljs-keyword">else</span>:
    x_train = x_train.reshape(x_train.shape[<span class="hljs-number">0</span>], img_rows, img_cols, <span class="hljs-number">1</span>)
    x_test = x_test.reshape(x_test.shape[<span class="hljs-number">0</span>], img_rows, img_cols, <span class="hljs-number">1</span>)
    input_shape = (img_rows, img_cols, <span class="hljs-number">1</span>)

x_train = x_train.astype(<span class="hljs-string">'float32'</span>)
x_test = x_test.astype(<span class="hljs-string">'float32'</span>)
x_train /= <span class="hljs-number">255</span>
x_test /= <span class="hljs-number">255</span>
print(<span class="hljs-string">'x_train shape:'</span>, x_train.shape)
print(x_train.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'train samples'</span>)
print(x_test.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'test samples'</span>)

<span class="hljs-comment"># Convert class vectors to binary class matrices</span>
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Conv2D(<span class="hljs-number">32</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>),
                activation=<span class="hljs-string">'relu'</span>,
                input_shape=input_shape))
model.add(Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
model.add(MaxPooling2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
model.add(Dropout(<span class="hljs-number">0.25</span>))
model.add(Flatten())
model.add(Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">'relu'</span>))
model.add(Dropout(<span class="hljs-number">0.5</span>))
model.add(Dense(num_classes, activation=<span class="hljs-string">'softmax'</span>))

<span class="hljs-comment"># Horovod: adjust learning rate based on number of GPUs.</span>
opt = keras.optimizers.Adadelta(<span class="hljs-number">1.0</span> * hvd.size())

<span class="hljs-comment"># Horovod: add Horovod Distributed Optimizer.</span>
opt = hvd.DistributedOptimizer(opt)

model.compile(loss=keras.losses.categorical_crossentropy,
            optimizer=opt,
            metrics=[<span class="hljs-string">'accuracy'</span>])

callbacks = [
    <span class="hljs-comment"># Horovod: broadcast initial variable states from rank 0 to all other processes.</span>
    <span class="hljs-comment"># This is necessary to ensure consistent initialization of all workers when</span>
    <span class="hljs-comment"># training is started with random weights or restored from a checkpoint.</span>
    hvd.callbacks.BroadcastGlobalVariablesCallback(<span class="hljs-number">0</span>),
]

<span class="hljs-comment"># Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.</span>
<span class="hljs-keyword">if</span> hvd.rank() == <span class="hljs-number">0</span>:
    callbacks.append(keras.callbacks.ModelCheckpoint(<span class="hljs-string">'./checkpoint-{epoch}.h5'</span>))

model.fit(x_train, y_train,
        batch_size=batch_size,
        callbacks=callbacks,
        epochs=epochs,
        verbose=<span class="hljs-number">1</span>,
        validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=<span class="hljs-number">0</span>)
print(<span class="hljs-string">'Test loss:'</span>, score[<span class="hljs-number">0</span>])
print(<span class="hljs-string">'Test accuracy:'</span>, score[<span class="hljs-number">1</span>])
</div></code></pre>
</li>
<li>
<p>利用horovodrun 执行分布式训练</p>
</li>
</ul>
<pre class="hljs"><code><div>horovodrun -np 16 -H server1:4,server2:4,server3:4,server4:4 python train.py
</div></code></pre>
<h2 id="5-%E6%80%BB%E7%BB%93">5. 总结</h2>
<p>本文分享了通过GPU利用率和分布式训练Horovod框架来提升深度学习训练。</p>
<ul>
<li>并行CPU加载数据和预处理，让GPU不再等待CPU</li>
<li>采用Horovod让数据并行来提高大数据量的训练的迭代时间</li>
</ul>

</body>
</html>
